import re
from urllib.parse import urlparse, urljoin, urldefrag
import urllib.robotparser as urllib_rp
from bs4 import BeautifulSoup
from textProcessor import TextProcessor
from hashlib import sha256
from collections import defaultdict
from utils import get_logger



class TextSimilarityProcessor:  #class for hashing (getting the fingerprint) of the webpage text and
                                #comparing it against previously crawled webpages to detect similarity and traps.

    previous_webpage_fingerprints = list()  #This class compares the current webpage fingerprint against previous webpage fingerprintss.

    def get_fingerprint(resp):  
    #gets the 32-bit fingerprint
        text = resp.get_text(strip=True)    #this function only uses the text of the html file to generate the fingerprint
        text_freq = TextProcessor.computeWordFrequencies(TextProcessor.tokenize(text))

        vector = [0] * 64
        for token, weight in text_freq.items():
            token_bit_hash = bin(int.from_bytes(sha256(token.encode()).digest(), 'big'))[-64:]    #generates a binary string for each token via
                                                                                                  #the sha256 hash function
        
            for i in range(64):     #if bit is 1, add the weight to the vector, otherwise subtract the weight
                if int(token_bit_hash[i]) == 1:
                    vector[i] += weight
                else:
                    vector[i] -= weight
        
        fingerprint = ''
        for i in vector:    #fingerprint is generated by making the nth bit 1 if the nth element in the vector is positive, 0 otherwise
            if i > 0:
                fingerprint += '1'
            else:
                fingerprint += '0'

        return fingerprint    

    def check_similar(resp):
        fingerprint = int(TextSimilarityProcessor.get_fingerprint(resp), 2)

        for previous_fingerprint in TextSimilarityProcessor.previous_webpage_fingerprints:      #checks current fingerprint against previous fingerprints
            similarity = 64 - (fingerprint ^ previous_fingerprint).bit_count()      #similarity counts the number of similar bits
            if (similarity / 64) > 0.95:        #set the threshold to 0.95
                return True

        TextSimilarityProcessor.previous_webpage_fingerprints.append(fingerprint)
        return False

class Robots:   #The Robots class checks if the url is allowed in their respective domain's robot.txt
    ics = urllib_rp.RobotFileParser()   #construct the RobotFileParser() object
    ics.set_url('https://www.ics.uci.edu/robots.txt')   #set the url of the robot.txt file
    ics.read()  #read the file
    cs = urllib_rp.RobotFileParser()
    cs.set_url('https://www.cs.uci.edu/robots.txt')
    cs.read()
    stat = urllib_rp.RobotFileParser()
    stat.set_url('https://www.stat.uci.edu/robots.txt')
    stat.read()
    informatics = urllib_rp.RobotFileParser()
    informatics.set_url('https://www.informatics.uci.edu/robots.txt')
    informatics.read()
    
    def check_robots(url, netloc):  #checks if url is allowed, given the url and its domain name
        if re.match(r".*\.ics\.uci\.edu.*", netloc):
            return Robots.ics.can_fetch('*', url)
        if re.match(r".*\.cs\.uci\.edu.*", netloc):
            return Robots.cs.can_fetch('*', url)
        if re.match(r".*\.stat\.uci\.edu.*", netloc):
            return Robots.stat.can_fetch('*', url)
        if re.match(r".*\.informatics\.uci\.edu.*", netloc):
            return Robots.informatics.can_fetch('*', url)

# This class will keep track of data needed for the report
class Report:
    longest_page = ("NaN", -1)    # holds the longest page length
    word_freq = defaultdict(int)    # word frequencies seen so far
    sub_domains = defaultdict(int) # sub domain as key and amount of pages found from sub domain as value
    seen_urls = set()   # set of urls seen
    N = 50   # top N words frequencies that would be presented
    logger = get_logger("REPORT")
    log_count = 1
    
    def __init__(self) -> None:
        pass

    def get_unique_pages(self):
        return len(self.seen_urls)  # returns amount of unique pages seen
    
    def get_longest_page(self):
        return self.longest_page    # returns length of longest page
    
    def get_N_common_words(self):
        return TextProcessor.getNTokenAndFreq(self.word_freq, self.N)   # gets the top N words with highest frequency
    
    def get_seen_URL_len(self): # gets total amount of unique urls seen
        return len(self.seen_urls)


    def update_sub_domains(self, url, num_of_pages): # takes a  url, along with the amount of pages found in that url and updates acoordingly
        if ".ics.uci.edu" not in url:
            return False # cannot be sub domain of ics.uci.edu
        
        parse_url = urlparse(url).hostname.split('.')

        if len(parse_url) <= 3: #checks if there is a subdomain
            return False #If no subdomain return False
        else:
            subURL = f"{urlparse(url).scheme}://{urlparse(url).hostname}" # Full subdomain URL
            self.sub_domains[subURL] = num_of_pages
            return True
            #print(".".join(parse_url[:-1])) #returns subdomain only if it is there is one or multiple


    def update_pages(self, url): # takes in a url, with or without fragment and adds it to the url set
        defrag_url, frag = urldefrag(url)

        self.seen_urls.add(defrag_url)     # adds a url without fragment to the url set of urls weve seen so far

    def update_longest_page(self, l, url):
        if l > self.longest_page[1]:
            self.longest_page = (url, l)   # updates longest length page if new page length is bigger

    def update_word_freq(self, wordFreq):
        for k, v in wordFreq.items():
            self.word_freq[k] += v  # update word freq of words weve seen so far with new words seen
    
    def write_data_to_file(self):
        with open("report.txt", "w") as output:
            # 1) Unique Pages Found
            output.write(f"TOTAL AMOUNT OF UNIQUE PAGES FOUND: {len(self.seen_urls)} \n")

            # 2) Longest Page In Terms of Words
            output.write(f"LONGEST PAGE: URL -> {self.longest_page[0]} / LENGTH -> {self.longest_page[1]}")

            # 3) 50 Most Common Words
            topNwords = self.get_N_common_words()
            output.write(f"\nTOP {self.N} WORDS: ")
            for pair in topNwords:
                output.write(f"\n\tWORD: {pair[0]} / FREQ: {pair[1]}")
            
            # 4) Sub domains Found
            sortedFreqBySD = sorted(self.sub_domains.items(), key=lambda x: (x[0]))
            output.write(f"\nSUB DOMAINS FOUND: ")
            for pair in sortedFreqBySD:
                output.write(f"\n\t{pair[0]}, {pair[1]}")
    
    def update_log(self):
        self.logger.info(f"_________________________________ LOG #{self.log_count} _________________________________")

        # 1) Unique Pages Found
        self.logger.info(f"TOTAL AMOUNT OF UNIQUE PAGES FOUND: {len(self.seen_urls)}")

        # 2) Longest Page In Terms of Words
        self.logger.info(f"LONGEST PAGE: URL -> {self.longest_page[0]} / LENGTH -> {self.longest_page[1]}")

        # 3) 50 Most Common Words
        self.logger.info(f"TOP {self.N} WORDS: ")
        topNwords = self.get_N_common_words()
        for pair in topNwords:
            self.logger.info(f"\tWORD: {pair[0]} / FREQ: {pair[1]}")

        # 4) Sub domains Found
        sortedFreqBySD = sorted(self.sub_domains.items(), key=lambda x: (x[0]))
        self.logger.info(f"SUB DOMAINS FOUND: ")
        for pair in sortedFreqBySD:
            self.logger.info(f"\t{pair[0]}, {pair[1]}")

        self.logger.info(f"___________________________________________________________________________________")
        
        self.log_count += 1
        
RT = Report()
        



def scraper(url, resp):
    links = extract_next_links(url, resp)
    return [link for link in links if is_valid(link)]

def extract_next_links(url, resp):
    # Implementation required.
    # url: the URL that was used to get the page
    # resp.url: the actual url of the page
    # resp.status: the status code returned by the server. 200 is OK, you got the page. Other numbers mean that there was some kind of problem.
    # resp.error: when status is not 200, you can check the error here, if needed.
    # resp.raw_response: this is where the page actually is. More specifically, the raw_response has two parts:
    #         resp.raw_response.url: the url, again
    #         resp.raw_response.content: the content of the page!
    # Return a list with the hyperlinks (as strings) scrapped from resp.raw_response.content

    num_unique_link = 0 # To count number of unique links
    url_set = set() #set with hyperlink to return

    if 200 <= resp.status < 300 and resp.raw_response is not None: #if status code is ok and it is a valid link
        
        # Updating Report
        RT.update_pages(url)

        soup = BeautifulSoup(resp.raw_response.content, 'lxml') #parser using beautiful soup

        # Updating Report
        page_str = soup.get_text() # page text
        words_in_page = TextProcessor.tokenizeWNoFilterCount(page_str)
        RT.update_longest_page(words_in_page, url)

        if TextSimilarityProcessor.check_similar(soup) == False: # checks for text similarity against previously added links

            # Updating Report
            word_freq = TextProcessor.computeWordFrequencies(TextProcessor.tokenize(page_str)) # get word freq
            RT.update_word_freq(word_freq)

            pages_found = 0
            for link in soup.find_all('a'):
                extracted_url = link.get('href')
                if extracted_url is None:  #check if extracted_url is a None object
                    continue

                index = extracted_url.rfind('#')
                url_remove_fragment = extracted_url[:index] if index >= 0 else extracted_url #removes the fragment portion of url
            
                absolute_url = urljoin(url, url_remove_fragment) #converts relative urls to absolute urls
                url_set.add(absolute_url) #adds url to list
                pages_found += 1
            
            # Updating Report
            if ".ics.uci.edu" in url: # if is a possible sub domain of ics.udi.edu
               RT.update_sub_domains(url, pages_found)


    redirect_codes = [301,302,303,307,308] #codes that are safe for redirects
    if resp.status in redirect_codes: #checks for redirects status code
        redirected_url = resp.raw_response_response.url #redirected url
        index = redirected_url.rfind('#')
        redirected_remove_fragment = redirected_url[:index] if index >= 0 else redirected_url #removes fragment portion of url
        url_set.add(redirected_remove_fragment) #adds redirect url to set

    # Updating Report
    RT.update_log()

    return list(url_set)

def is_valid(url):
    # Decide whether to crawl this url or not. 
    # If you decide to crawl it, return True; otherwise return False.
    # There are already some conditions that return False.
    try:
        parsed = urlparse(url)
        if parsed.scheme not in set(["http", "https"]):
            return False
        if check_domain_robots(url, parsed):
            return not re.match(
                r".*\.(css|js|bmp|gif|jpe?g|ico"
                + r"|png|tiff?|mid|mp2|mp3|mp4"
                + r"|wav|avi|mov|mpeg|ram|m4v|mkv|ogg|ogv|pdf"
                + r"|ps|eps|tex|ppt|pptx|doc|docx|xls|xlsx|names"
                + r"|data|dat|exe|bz2|tar|msi|bin|7z|psd|dmg|iso"
                + r"|epub|dll|cnf|tgz|sha1|war|txt|json"
                + r"|thmx|mso|arff|rtf|jar|csv"
                + r"|rm|smil|wmv|swf|wma|zip|rar|gz)$", parsed.path.lower())
        else:
            return False
    except TypeError:
        print ("TypeError for ", parsed)
        raise

def check_domain_robots(url, url_parse):
    # checks if the url is under one of the four domains and is disallowed by their robots.txt

    netloc = url_parse.netloc.lower()
    
    #checks if the hostname is one of the four valid domains. Then checks their robots.txt
    if re.match(r".*\.ics\.uci\.edu.*|.*\.cs\.uci\.edu.*|.*\.stat\.uci\.edu.*|.*\.informatics\.uci\.edu.*", netloc):
        return Robots.check_robots(url, netloc)
    return False
